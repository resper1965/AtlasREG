# üîç Como Funciona a Busca de Not√≠cias no AtlasReg

**Powered by:** ness. (Montserrat Medium, ponto em #00ADE8)

---

## üìä VIS√ÉO GERAL DO SISTEMA DE COLETA

O AtlasReg coleta not√≠cias automaticamente de **m√∫ltiplas fontes** usando **web scraping inteligente**.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FONTES EXTERNAS (Sites P√∫blicos)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ ANEEL (gov.br/aneel)                             ‚îÇ
‚îÇ  ‚Ä¢ ONS (ons.org.br)                                 ‚îÇ
‚îÇ  ‚Ä¢ SIGEL (sigel.aneel.gov.br)                       ‚îÇ
‚îÇ  ‚Ä¢ Canal Energia (canalenergia.com.br)              ‚îÇ
‚îÇ  ‚Ä¢ MegaWhat (megawhat.energy)                       ‚îÇ
‚îÇ  ‚Ä¢ EPBR (epbr.com.br)                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îÇ SCRAPING AUTOMATIZADO
               ‚îÇ (Airflow DAGs + Scrapy/Playwright)
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COLETA & ARMAZENAMENTO                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. Scrapers visitam sites diariamente              ‚îÇ
‚îÇ  2. Extraem: T√≠tulo, Data, Corpo, URL              ‚îÇ
‚îÇ  3. Baixam PDFs se necess√°rio                       ‚îÇ
‚îÇ  4. Salvam em MinIO (object storage)                ‚îÇ
‚îÇ  5. Registram metadata no PostgreSQL                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îÇ PROCESSAMENTO
               ‚îÇ (Pipeline de IA)
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TRANSFORMA√á√ÉO INTELIGENTE                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. PDF ‚Üí Texto (PDFMiner + OCR)                    ‚îÇ
‚îÇ  2. Classifica√ß√£o (BERTimbau)                       ‚îÇ
‚îÇ     ‚îî‚îÄ Tipo: Multa, Decis√£o, Transa√ß√£o, etc.       ‚îÇ
‚îÇ  3. Extra√ß√£o de Entidades (spaCy)                   ‚îÇ
‚îÇ     ‚îî‚îÄ Empresas, CNPJs, Valores, Ativos            ‚îÇ
‚îÇ  4. Indexa√ß√£o                                       ‚îÇ
‚îÇ     ‚îú‚îÄ Sem√¢ntica (SBERT + FAISS)                    ‚îÇ
‚îÇ     ‚îî‚îÄ Full-text (Elasticsearch)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îÇ DISPONIBILIZA√á√ÉO
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  USU√ÅRIO FINAL                                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Dashboard com feed de eventos                    ‚îÇ
‚îÇ  ‚Ä¢ Busca avan√ßada (filters, keywords)               ‚îÇ
‚îÇ  ‚Ä¢ Alertas por email (daily digest)                 ‚îÇ
‚îÇ  ‚Ä¢ Watchlists personalizadas                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üï∑Ô∏è PARTE 1: WEB SCRAPING (Coleta Autom√°tica)

### O Que √© Web Scraping?

**Web Scraping = "Rob√¥" que visita sites e extrai informa√ß√µes**

Imagine um assistente que:
1. Abre o navegador
2. Vai no site da ANEEL
3. Procura not√≠cias novas
4. Copia t√≠tulo, data, texto completo
5. Salva tudo estruturado
6. Faz isso **automaticamente todos os dias**

### Exemplo: Scraper da ANEEL

#### Site Alvo
```
https://www.gov.br/aneel/pt-br/assuntos/noticias
```

#### O Que o Scraper Faz

**1. Visita a p√°gina de not√≠cias:**
```python
# apps/scraper/scrapers/aneel_scraper.py
import scrapy

class AneelNewsSpider(scrapy.Spider):
    name = "aneel_news"
    start_urls = [
        'https://www.gov.br/aneel/pt-br/assuntos/noticias'
    ]
    
    def parse(self, response):
        # Extrai cada not√≠cia da p√°gina
        for article in response.css('div.item-noticia'):
            yield {
                'title': article.css('h2.titulo::text').get(),
                'date': article.css('span.data::text').get(),
                'url': article.css('a::attr(href)').get(),
                'summary': article.css('p.resumo::text').get()
            }
```

**2. Segue link de cada not√≠cia:**
```python
def parse_article(self, response):
    # Extrai conte√∫do completo
    return {
        'title': response.css('h1::text').get(),
        'date': response.css('time::attr(datetime)').get(),
        'body': response.css('div.conteudo').get(),
        'url': response.url,
        'source': 'ANEEL'
    }
```

**3. Salva no MinIO:**
```python
import json
from minio import Minio

def save_to_minio(article):
    minio = Minio('localhost:19000',
                  access_key='admin',
                  secret_key='atlasreg2025',
                  secure=False)
    
    # Converter para JSON
    data = json.dumps(article, ensure_ascii=False)
    
    # Salvar em bucket
    filename = f"aneel/news/{article['date']}/{article['title']}.json"
    minio.put_object(
        'raw-documents',
        filename,
        data,
        len(data)
    )
```

**4. Registra no PostgreSQL:**
```python
# Metadata para tracking
document = Document(
    filename=filename,
    source='ANEEL',
    doc_type='news',
    status='raw',  # Ainda n√£o processado
    scraped_at=datetime.now(),
    url=article['url']
)
db.add(document)
db.commit()
```

---

## ü§ñ PARTE 2: ORQUESTRA√á√ÉO (Quando Buscar)

### Airflow: Agendamento Autom√°tico

**Airflow = "Rel√≥gio inteligente" que dispara scrapers**

#### DAG (Directed Acyclic Graph)

```python
# apps/scraper/dags/aneel_news_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Configura√ß√£o do DAG
default_args = {
    'owner': 'atlasreg',
    'depends_on_past': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'aneel_news_scraper',
    default_args=default_args,
    description='Coleta not√≠cias da ANEEL diariamente',
    schedule_interval='0 6 * * *',  # Todo dia √†s 06:00 BRT
    start_date=datetime(2025, 10, 17),
    catchup=False
)

# Task: Rodar scraper
scrape_task = PythonOperator(
    task_id='scrape_aneel_news',
    python_callable=run_aneel_scraper,
    dag=dag
)
```

**Resultado:** Todos os dias √†s 06:00, automaticamente:
1. Airflow acorda
2. Dispara o scraper ANEEL
3. Scraper coleta not√≠cias novas
4. Salva tudo no MinIO + PostgreSQL
5. Dispara processamento (IA)

---

## üß† PARTE 3: PROCESSAMENTO INTELIGENTE

### BERTimbau: Classifica√ß√£o Autom√°tica

**O Que Faz:** L√™ o texto e identifica o TIPO de evento

```python
from transformers import pipeline

# Modelo treinado para portugu√™s brasileiro
classifier = pipeline(
    "text-classification",
    model="models/event_classifier"  # BERTimbau fine-tuned
)

# Processar not√≠cia
text = """
ANEEL aplica multa de R$ 2 milh√µes √† Transmissora X 
por descumprimento de indicadores de continuidade...
"""

result = classifier(text)
# Resultado:
# {
#   'label': 'MULTA',       ‚Üê Tipo identificado
#   'score': 0.92           ‚Üê 92% de confian√ßa
# }
```

**Categorias poss√≠veis:**
- `MULTA` - Penalidades da ANEEL
- `DECISAO` - Decis√µes regulat√≥rias
- `TRANSACAO` - M&A, compra/venda de ativos
- `INCIDENTE` - Quedas de torre, desligamentos
- `NOTICIA` - Not√≠cia gen√©rica
- `OUTRO` - N√£o classificado

### spaCy: Extra√ß√£o de Entidades

**O Que Faz:** Identifica nomes, valores, datas no texto

```python
import spacy

nlp = spacy.load("pt_core_news_lg")
doc = nlp(text)

# Extrair entidades
entities = []
for ent in doc.ents:
    entities.append({
        'type': ent.label_,  # COMPANY, MONEY, DATE
        'text': ent.text,    # "Transmissora X", "R$ 2 milh√µes"
        'start': ent.start_char,
        'end': ent.end_char
    })

# Regex customizado para CNPJ
import re
cnpj_pattern = r'\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}'
cnpjs = re.findall(cnpj_pattern, text)
```

**Resultado estruturado:**
```json
{
  "title": "ANEEL aplica multa de R$ 2M...",
  "event_type": "MULTA",
  "confidence": 0.92,
  "entities": [
    {"type": "COMPANY", "text": "Transmissora X"},
    {"type": "CNPJ", "text": "12.345.678/0001-90"},
    {"type": "MONEY", "text": "R$ 2 milh√µes"},
    {"type": "DATE", "text": "15 de outubro de 2025"}
  ],
  "source": "ANEEL",
  "url": "https://..."
}
```

---

## üîé PARTE 4: BUSCA PARA O USU√ÅRIO

### Dois Tipos de Busca

#### 1. Busca Sem√¢ntica (FAISS)

**Como funciona:** Busca por **significado**, n√£o palavras exatas

```python
from sentence_transformers import SentenceTransformer
import faiss

# Gerar embedding da query do usu√°rio
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

query = "multas por atraso em obras"
query_embedding = model.encode([query])

# Buscar no √≠ndice FAISS
index = faiss.read_index("events.index")
distances, indices = index.search(query_embedding, k=10)

# Retornar os 10 eventos mais similares CONCEITUALMENTE
similar_events = [events[i] for i in indices[0]]
```

**Exemplo:**
- **Query:** "penalidades da ANEEL"
- **Encontra:**
  - "ANEEL aplica multa de R$ 2M..."
  - "Superintend√™ncia autuou empresa por..."
  - "Processo de fiscaliza√ß√£o resultou em..."
  
**Mesmo sem usar palavra "multa" exata!**

#### 2. Busca Full-Text (Elasticsearch)

**Como funciona:** Busca por **palavras-chave** com filtros

```python
from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:19200'])

# Query com filtros
query = {
    "query": {
        "bool": {
            "must": [
                {"match": {"text": "multa"}}
            ],
            "filter": [
                {"term": {"event_type": "MULTA"}},
                {"term": {"company.keyword": "Transmissora X"}},
                {"range": {"amount": {"gte": 1000000}}}
            ]
        }
    }
}

results = es.search(index="events", body=query)
```

**Exemplo:**
- **Filtros:** 
  - Tipo: MULTA
  - Empresa: Transmissora X
  - Valor: > R$ 1 milh√£o
  - Data: √∫ltimos 30 dias
  
- **Resultados:** Apenas eventos que correspondem a TODOS os filtros

---

## üï∞Ô∏è FREQU√äNCIA DE COLETA

### Agendamento por Fonte

| Fonte | Frequ√™ncia | Hor√°rio | Documentos/dia |
|-------|-----------|---------|----------------|
| **ANEEL Not√≠cias** | Di√°ria | 06:00 BRT | 5-10 |
| **ANEEL Despachos** | Di√°ria | 07:00 BRT | 20-50 |
| **ANEEL PVs (Multas)** | Semanal | Seg 08:00 | 10-20 |
| **ONS Ocorr√™ncias** | Di√°ria | 06:30 BRT | 5-15 |
| **SIGEL Dados** | Mensal | Dia 1, 09:00 | 100-200 |
| **Canal Energia** | Di√°ria | 09:00 BRT | 10-20 |
| **MegaWhat** | Di√°ria | 09:30 BRT | 5-10 |
| **EPBR** | Di√°ria | 10:00 BRT | 5-10 |

**Total:** ~60-135 documentos/dia em m√©dia

### Pipeline Completo

```
06:00 ‚Üí ANEEL News scraper inicia
06:05 ‚Üí Not√≠cias salvas no MinIO
06:10 ‚Üí Celery worker processa (IA)
06:15 ‚Üí Eventos aparecem no dashboard
06:30 ‚Üí ONS scraper inicia
...
08:00 ‚Üí Daily digest email enviado aos usu√°rios
```

---

## üõ†Ô∏è TECNOLOGIAS DE SCRAPING

### 1. Scrapy (Sites Est√°ticos)

**Para:** Sites HTML simples (ANEEL, ONS)

```python
# Vantagens:
‚úÖ R√°pido (async)
‚úÖ Rate limiting built-in
‚úÖ Retry autom√°tico
‚úÖ Extra√ß√£o de dados f√°cil (CSS selectors)

# Exemplo ANEEL
import scrapy

class AneelSpider(scrapy.Spider):
    custom_settings = {
        'DOWNLOAD_DELAY': 5,  # 5s entre requests (√©tico)
        'USER_AGENT': 'AtlasReg/1.0 (+https://atlasreg.com)'
    }
    
    def parse(self, response):
        for noticia in response.css('article.noticia'):
            yield {
                'titulo': noticia.css('h2::text').get(),
                'data': noticia.css('time::attr(datetime)').get(),
                'url': noticia.css('a::attr(href)').get()
            }
```

### 2. Playwright (Sites JavaScript)

**Para:** Sites que carregam conte√∫do via JavaScript (alguns portais de m√≠dia)

```python
# Vantagens:
‚úÖ Renderiza JavaScript
‚úÖ Simula navegador real
‚úÖ Espera elementos carregarem
‚úÖ Screenshots para debug

# Exemplo Canal Energia
from playwright.sync_api import sync_playwright

def scrape_canal_energia():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        page.goto('https://www.canalenergia.com.br/noticias/')
        page.wait_for_selector('article.noticia')
        
        # Extrair not√≠cias ap√≥s JS carregar
        noticias = page.query_selector_all('article.noticia')
        
        for noticia in noticias:
            titulo = noticia.query_selector('h3').inner_text()
            data = noticia.query_selector('time').get_attribute('datetime')
            url = noticia.query_selector('a').get_attribute('href')
            
            yield {'titulo': titulo, 'data': data, 'url': url}
        
        browser.close()
```

### 3. Airflow (Orquestra√ß√£o)

**Para:** Agendar e monitorar scrapers

```python
# Airflow UI: http://localhost:8200
# Visualiza:
# - Quais scrapers rodaram
# - Quantos documentos coletados
# - Erros e retries
# - Hist√≥rico de execu√ß√µes
```

---

## üìë EXEMPLO COMPLETO: Fluxo de 1 Not√≠cia

### 1. Scraping (06:00 BRT)

**ANEEL publica not√≠cia:**
```
T√≠tulo: "ANEEL aprova reajuste tarif√°rio de 8,5% para transmissoras"
URL: https://www.gov.br/aneel/pt-br/assuntos/noticias/2025/aneel-aprova-reajuste...
Data: 17/10/2025 05:00
Corpo: A Ag√™ncia Nacional de Energia El√©trica (ANEEL) aprovou...
```

**Scraper detecta e coleta:**
```python
# 06:05 - Airflow DAG dispara
# 06:06 - Scrapy visita p√°gina
# 06:07 - Extrai dados
{
    "title": "ANEEL aprova reajuste tarif√°rio de 8,5%...",
    "date": "2025-10-17T05:00:00",
    "url": "https://...",
    "body": "A Ag√™ncia Nacional...",
    "source": "ANEEL",
    "scraped_at": "2025-10-17T06:07:00"
}

# 06:08 - Salva em MinIO
# Path: raw-documents/aneel/news/2025-10-17/reajuste-tarifario.json

# 06:08 - Registra no PostgreSQL (tabela documents)
```

### 2. Processamento (06:10 BRT)

**Celery worker pega documento:**
```python
# Worker 1: Classifica√ß√£o
classifier = pipeline("text-classification", model="BERTimbau")
result = classifier("ANEEL aprova reajuste tarif√°rio...")

# Output: {'label': 'DECISAO', 'score': 0.89}

# Worker 2: Extra√ß√£o de entidades
nlp = spacy.load("pt_core_news_lg")
doc = nlp("A ANEEL aprovou reajuste de 8,5%...")

# Entidades encontradas:
# - ORG: "ANEEL"
# - MONEY: "8,5%"
# - DATE: "17/10/2025"
# - Empresas: [detecta se mencionar nomes espec√≠ficos]

# Worker 3: Indexa√ß√£o
# - Gera embedding sem√¢ntico (SBERT)
# - Adiciona ao √≠ndice FAISS
# - Indexa no Elasticsearch

# Worker 4: Salva evento no PostgreSQL
event = Event(
    title="ANEEL aprova reajuste tarif√°rio de 8,5%",
    summary="A ANEEL aprovou...",
    event_type="DECISAO",
    date=datetime(2025, 10, 17, 5, 0),
    source="ANEEL",
    source_url="https://...",
    confidence_score=0.89,
    created_at=datetime.now()
)
```

### 3. Disponibiliza√ß√£o (06:15 BRT)

**Usu√°rio acessa dashboard:**
```tsx
// Frontend busca eventos
fetch('http://localhost:8100/events/search?limit=20')

// API retorna
{
  "items": [
    {
      "id": "evt_123",
      "title": "ANEEL aprova reajuste tarif√°rio de 8,5%",
      "event_type": "DECISAO",
      "date": "2025-10-17T05:00:00Z",
      "source": "ANEEL",
      "confidence": 0.89,
      "is_new": true  ‚Üê Badge "NOVO" no card
    },
    // ... mais eventos
  ]
}

// Dashboard renderiza
<EventCard 
  title="ANEEL aprova reajuste..." 
  badge="DECIS√ÉO"
  isNew={true}
/>
```

### 4. Alertas (08:00 BRT)

**Daily Digest enviado:**

Se usu√°rio tem watchlist com:
- Keywords: ["reajuste", "tarifa"]
- Tipos: ["DECISAO"]

**Recebe email:**
```html
<h2>Seu Resumo Di√°rio - AtlasReg by ness.</h2>

<h3>1 novo evento encontrado:</h3>

<div style="border-left: 3px solid #00ADE8;">
  <strong>ANEEL aprova reajuste tarif√°rio de 8,5%</strong>
  <p>Tipo: DECIS√ÉO | Data: 17/10/2025</p>
  <p>A Ag√™ncia Nacional...</p>
  <a href="https://atlasreg.com/events/evt_123">Ver Detalhes ‚Üí</a>
</div>

<footer>Powered by ness.</footer>
```

---

## üéØ EXEMPLO REAL: Como Funciona na Pr√°tica

### Cen√°rio: Analista quer monitorar multas

#### Passo 1: Configurar Watchlist

```tsx
// Dashboard ‚Üí Minhas Watchlists ‚Üí Nova Watchlist
{
  "name": "Multas e Penalidades",
  "event_types": ["MULTA"],
  "companies": ["Transmissora X", "Transmissora Y"],
  "keywords": ["fiscaliza√ß√£o", "penalidade"]
}
```

#### Passo 2: Sistema Trabalha Automaticamente

**Dia 1 (Segunda):**
- 06:00 - Scrapers coletam 60+ documentos
- 06:30 - IA processa tudo
- 07:00 - 3 multas detectadas
- 08:00 - Email enviado: "3 novos eventos na sua watchlist Multas"

**Dia 2 (Ter√ßa):**
- 06:00 - Scrapers coletam 70 documentos
- 06:30 - IA processa
- 07:00 - 0 multas detectadas
- 08:00 - Email N√ÉO enviado (sem eventos novos)

**Dia 3 (Quarta):**
- 06:00 - Scrapers coletam 55 documentos
- 06:15 - **ALERTA CR√çTICO:** Multa de R$ 5M detectada
- 06:16 - **Email instant√¢neo enviado** (urgente!)
- 08:00 - Email digest normal tamb√©m

#### Passo 3: Analista Acessa Dashboard

```
Dashboard (http://localhost:3000/dashboard)
‚îú‚îÄ Feed de Eventos
‚îÇ  ‚îú‚îÄ [MULTA] [NOVO] ANEEL aplica R$ 5M... ‚Üê Cr√≠tico
‚îÇ  ‚îú‚îÄ [DECIS√ÉO] Aprovado reajuste...
‚îÇ  ‚îî‚îÄ [NOT√çCIA] Setor de transmiss√£o...
‚îÇ
‚îú‚îÄ Filtros (Sidebar)
‚îÇ  ‚îú‚îÄ ‚òë Multa
‚îÇ  ‚îú‚îÄ ‚òê Decis√£o
‚îÇ  ‚îî‚îÄ Empresa: [Transmissora X ‚ñº]
‚îÇ
‚îî‚îÄ Busca
   "multas transmissora X √∫ltimos 30 dias"
   ‚Üí 5 resultados encontrados
```

---

## üìã FONTES DE DADOS - DETALHAMENTO

### 1. ANEEL (Ag√™ncia Nacional de Energia El√©trica)

**URL Base:** https://www.gov.br/aneel/

**O Que Coletamos:**

#### a) Not√≠cias
```
URL: /pt-br/assuntos/noticias
Formato: HTML
Atualiza√ß√£o: Di√°ria
Scraper: Scrapy
Campos: T√≠tulo, Data, Resumo, URL
```

#### b) Despachos
```
URL: /pt-br/assuntos/despachos
Formato: PDF listados em HTML
Atualiza√ß√£o: Di√°ria
Scraper: Scrapy + PDFMiner
Campos: N√∫mero, Data, Assunto, PDF
```

#### c) Processos de Fiscaliza√ß√£o (PVs)
```
URL: /pt-br/centrais-de-conteudos/processos-de-fiscalizacao
Formato: Tabela HTML + PDFs
Atualiza√ß√£o: Semanal
Scraper: Scrapy
Campos: N√∫mero do PV, Empresa, Valor Multa, Data, PDF
```

### 2. ONS (Operador Nacional do Sistema)

**URL Base:** https://www.ons.org.br/

**O Que Coletamos:**

```
a) Ocorr√™ncias Operacionais
   URL: /paginas/energia-agora/ocorrencias
   Formato: HTML/JSON
   Info: Desligamentos, quedas, incidentes
   
b) Boletins Di√°rios
   URL: /paginas/servicos/publicacoes/boletins
   Formato: PDF
   Info: Opera√ß√£o do sistema, carga, gera√ß√£o
```

### 3. SIGEL (Sistema de Informa√ß√µes Geogr√°ficas)

**URL Base:** https://sigel.aneel.gov.br/

**O Que Coletamos:**

```
Dados Cadastrais de Agentes
URL: /portal/home/index.html
Formato: Tabelas HTML
Info: Nome, CNPJ, Grupo Econ√¥mico, Outorgas
Atualiza√ß√£o: Mensal
```

### 4. M√≠dia Especializada

#### Canal Energia
```
URL: https://www.canalenergia.com.br/noticias/
Scraper: Playwright (site JS-heavy)
Categorias: Transmiss√£o, Regula√ß√£o, M&A
```

#### MegaWhat
```
URL: https://megawhat.energy/
Scraper: Scrapy
Foco: An√°lises t√©cnicas, mercado
```

#### EPBR
```
URL: https://epbr.com.br/
Scraper: Scrapy (respeitar paywall)
Foco: Transa√ß√µes, investimentos
```

---

## üîß IMPLEMENTA√á√ÉO DO SCRAPER (C√≥digo Real)

Vou criar um scraper completo da ANEEL como exemplo:


