# ğŸš€ ImplementaÃ§Ã£o Scrapy-Playwright - Guia Passo a Passo

**BMad Orchestrator - Plano de ExecuÃ§Ã£o**  
**Objetivo:** Completar 10/10 scrapers funcionais em 2-3 dias

---

## ğŸ“‹ CHECKLIST DE IMPLEMENTAÃ‡ÃƒO

### âœ… FASE 1: Setup (2-3 horas)

**1.1 Instalar DependÃªncias**
```bash
cd /home/resper/nSaulo/apps/scraper

# Ativar venv
source .venv/bin/activate

# Instalar scrapy-playwright
pip install scrapy-playwright

# Instalar navegador
playwright install chromium

# Verificar
playwright --version
```

**1.2 Atualizar requirements.txt**
```bash
echo "scrapy-playwright==0.0.34" >> requirements.txt
```

---

### âœ… FASE 2: CÃ³digo Base (3-4 horas)

**2.1 Criar Hybrid Spider**

Arquivo: `apps/scraper/scrapers/hybrid_scraper.py`

```python
"""
Hybrid Scraper - Scrapy + Playwright
Usa Playwright apenas quando necessÃ¡rio (requires_javascript: true)
"""

from scrapy_playwright.page import PageMethod
import scrapy
import yaml
from pathlib import Path
from datetime import datetime

class HybridSpider(scrapy.Spider):
    """Spider hÃ­brido que usa Playwright quando necessÃ¡rio"""
    
    def __init__(self, source_id: str, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # Carregar config
        config_path = Path(__file__).parent.parent / 'config' / 'sources.yaml'
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # Encontrar source
        self.source_config = None
        for source in config['sources']:
            if source['id'] == source_id:
                self.source_config = source
                break
        
        if not self.source_config or not self.source_config.get('enabled'):
            raise ValueError(f"Source {source_id} not found or disabled")
        
        # Configurar spider
        self.name = source_id
        self.source_id = source_id
        self.source_name = self.source_config['name']
        self.selectors = self.source_config.get('selectors', {})
        self.requires_js = self.source_config.get('requires_javascript', False)
        
        # URLs
        urls = self.source_config['urls']
        if 'noticias' in urls:
            self.start_urls = [urls['noticias']]
        elif 'lista' in urls:
            self.start_urls = [urls['lista']]
        else:
            self.start_urls = [urls['base']]
    
    @property
    def custom_settings(self):
        settings = {
            'DOWNLOAD_DELAY': self.source_config.get('rate_limit', 5),
            'CONCURRENT_REQUESTS': 1,
            'USER_AGENT': 'AtlasReg/1.0 by ness. (https://atlasreg.com)',
            'ROBOTSTXT_OBEY': True,
        }
        
        # Se requer JS, adicionar Playwright
        if self.requires_js:
            settings.update({
                'DOWNLOAD_HANDLERS': {
                    'https': 'scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler',
                    'http': 'scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler',
                },
                'PLAYWRIGHT_LAUNCH_OPTIONS': {
                    'headless': True,
                    'args': [
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-blink-features=AutomationControlled'
                    ]
                },
                'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 30000,
            })
        
        return settings
    
    def start_requests(self):
        """Gerar requests iniciais"""
        for url in self.start_urls:
            if self.requires_js:
                # Playwright mode
                yield scrapy.Request(
                    url,
                    meta={
                        'playwright': True,
                        'playwright_page_methods': [
                            PageMethod('wait_for_load_state', 'networkidle'),
                            PageMethod('wait_for_selector', 
                                      self.selectors.get('lista', 'article'),
                                      timeout=10000)
                        ],
                        'playwright_include_page': True
                    },
                    callback=self.parse_with_playwright,
                    errback=self.errback
                )
            else:
                # Scrapy normal
                yield scrapy.Request(url, callback=self.parse)
    
    def parse(self, response):
        """Parse com Scrapy normal (sites estÃ¡ticos)"""
        lista_selector = self.selectors.get('lista', 'article')
        items = response.css(lista_selector)
        
        self.logger.info(f"ğŸ“° {self.source_name}: {len(items)} items (Scrapy)")
        
        for item in items:
            titulo = self.extract_text(item, self.selectors.get('titulo'))
            url = self.extract_attr(item, self.selectors.get('url'), 'href')
            data = self.extract_text(item, self.selectors.get('data'))
            resumo = self.extract_text(item, self.selectors.get('resumo'))
            
            if titulo and url:
                url_completa = response.urljoin(url)
                
                if self.should_collect(titulo, resumo):
                    yield self.create_item(titulo, url_completa, data, resumo)
    
    async def parse_with_playwright(self, response):
        """Parse com Playwright (sites JavaScript)"""
        page = response.meta['playwright_page']
        
        # Scroll para lazy loading
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await page.wait_for_timeout(2000)
        
        # Extrair items
        lista_selector = self.selectors.get('lista', 'article')
        items = await page.query_selector_all(lista_selector)
        
        self.logger.info(f"ğŸ“° {self.source_name}: {len(items)} items (Playwright)")
        
        results = []
        for item in items:
            # Extrair tÃ­tulo
            titulo_sel = self.selectors.get('titulo', 'h2')
            titulo_elem = await item.query_selector(titulo_sel.split('::')[0])
            titulo = await titulo_elem.inner_text() if titulo_elem else None
            
            # Extrair URL
            url_sel = self.selectors.get('url', 'a')
            url_elem = await item.query_selector(url_sel.split('::')[0])
            url = await url_elem.get_attribute('href') if url_elem else None
            
            # Extrair data
            data_sel = self.selectors.get('data', 'time')
            data_elem = await item.query_selector(data_sel.split('::')[0])
            data = await data_elem.inner_text() if data_elem else None
            
            # Extrair resumo
            resumo_sel = self.selectors.get('resumo', 'p')
            resumo_elem = await item.query_selector(resumo_sel.split('::')[0])
            resumo = await resumo_elem.inner_text() if resumo_elem else None
            
            if titulo and url:
                url_completa = response.urljoin(url)
                
                if self.should_collect(titulo, resumo):
                    results.append(self.create_item(titulo, url_completa, data, resumo))
        
        await page.close()
        
        for result in results:
            yield result
    
    def extract_text(self, selector, css):
        """Extrair texto com fallback"""
        if not css:
            return None
        
        # Tentar mÃºltiplos seletores (separados por vÃ­rgula)
        for sel in css.split(','):
            sel = sel.strip()
            if '::text' in sel:
                sel = sel.replace('::text', '::text').strip()
                result = selector.css(sel).get()
            else:
                result = selector.css(f"{sel}::text").get()
            
            if result:
                return result.strip()
        
        return None
    
    def extract_attr(self, selector, css, attr):
        """Extrair atributo com fallback"""
        if not css:
            return None
        
        for sel in css.split(','):
            sel = sel.strip()
            if '::attr' in sel:
                result = selector.css(sel).get()
            else:
                result = selector.css(f"{sel}::attr({attr})").get()
            
            if result:
                return result.strip()
        
        return None
    
    def should_collect(self, titulo: str, resumo: str = '') -> bool:
        """Verificar se item deve ser coletado"""
        filters = self.source_config.get('filters', {})
        keywords = filters.get('keywords', [])
        
        if not keywords:
            return True
        
        text = f"{titulo} {resumo}".lower()
        return any(kw.lower() in text for kw in keywords)
    
    def create_item(self, titulo, url, data, resumo=''):
        """Criar item estruturado"""
        return {
            'title': titulo,
            'url': url,
            'date': data,
            'summary': resumo,
            'source': self.source_name,
            'source_id': self.source_id,
            'scraped_at': datetime.now().isoformat(),
        }
    
    def errback(self, failure):
        """Tratar erros"""
        self.logger.error(f"âŒ Erro: {failure}")
```

---

### âœ… FASE 3: Testar (2-3 horas)

**3.1 Testar sites Playwright**

```bash
# ANEEL (gov.br - JS)
scrapy crawl aneel_noticias -o test_aneel.json

# MME (gov.br - JS)
scrapy crawl mme -o test_mme.json

# ONS (JS heavy)
scrapy crawl ons_ocorrencias -o test_ons.json
```

**3.2 Testar sites Scrapy**

```bash
# MegaWhat (estÃ¡tico)
scrapy crawl megawhat -o test_megawhat.json

# EPBR (estÃ¡tico)
scrapy crawl epbr -o test_epbr.json
```

**3.3 Verificar outputs**

```bash
# Ver quantos items coletados
cat test_aneel.json | jq length

# Ver primeiro item
cat test_aneel.json | jq '.[0]'

# Ver tÃ­tulos
cat test_aneel.json | jq '.[].title'
```

---

### âœ… FASE 4: Ajustes Finais (2-3 horas)

**4.1 Ajustar URLs Quebradas**

```yaml
# sources.yaml - Corrigir placeholders

# ANEEL Despachos - remover {ano}
- id: aneel_despachos
  urls:
    lista: "https://www.gov.br/aneel/pt-br/acesso-a-informacao/biblioteca/despachos/2025"

# ANEEL PVs - URL correta
- id: aneel_pv
  urls:
    lista: "https://fiscalizacao.aneel.gov.br/Processos/ConsultaProcessual"
```

**4.2 Aumentar Timeouts**

```yaml
# EPE - site lento
- id: epe
  rate_limit: 15  # Aumentar de 5 para 15
  timeout: 60     # Adicionar timeout maior
```

**4.3 Ajustar Selectors**

```bash
# Testar cada fonte manualmente
# Inspecionar HTML real
# Ajustar selectors conforme necessÃ¡rio
```

---

### âœ… FASE 5: ValidaÃ§Ã£o Final (1 hora)

**5.1 Re-executar Validation Script**

```bash
cd apps/scraper
source .venv/bin/activate
python test_scrapers.py
```

**Resultado Esperado:**
```
================================================================================
ğŸ“Š SUMÃRIO DE VALIDAÃ‡ÃƒO
================================================================================

âœ… OK - ANEEL - NotÃ­cias
âœ… OK - ANEEL - Despachos
âœ… OK - ANEEL - PVs
âœ… OK - ONS - OcorrÃªncias
âœ… OK - SIGEL
âœ… OK - Canal Energia
âœ… OK - MegaWhat Energy
âœ… OK - EPBR
âœ… OK - ABRATE
âœ… OK - MME
âš ï¸  OK - EPE (lento mas funciona)

================================================================================
âœ… Validados: 11/11
âŒ Falharam: 0/11

ğŸ‰ TODOS OS SCRAPERS VALIDADOS!
================================================================================
```

---

## ğŸ“Š TIMELINE

```
DIA 1 (6h):
â”œâ”€ 09:00-12:00 â†’ FASE 1: Setup + FASE 2: CÃ³digo
â””â”€ 14:00-17:00 â†’ FASE 3: Testes iniciais

DIA 2 (8h):
â”œâ”€ 09:00-12:00 â†’ FASE 4: Ajustes (URLs, selectors)
â”œâ”€ 14:00-17:00 â†’ FASE 4: Mais ajustes
â””â”€ 17:00-18:00 â†’ FASE 5: ValidaÃ§Ã£o final

DIA 3 (4h - buffer):
â””â”€ 09:00-13:00 â†’ CorreÃ§Ãµes finais, documentaÃ§Ã£o
```

**Total:** 16-20 horas (2-3 dias)

---

## âœ… CRITÃ‰RIOS DE SUCESSO

- [x] Scrapy-Playwright instalado
- [ ] 10/11 scrapers validados (90%+)
- [ ] Dados reais coletados em JSON
- [ ] Performance aceitÃ¡vel (<10s/fonte)
- [ ] Zero erros em 3 execuÃ§Ãµes consecutivas

---

## ğŸš€ PRÃ“XIMA AÃ‡ÃƒO

**EXECUTAR AGORA:**

```bash
cd /home/resper/nSaulo/apps/scraper
source .venv/bin/activate
pip install scrapy-playwright
playwright install chromium
```

**Criar:** `hybrid_scraper.py` (copiar cÃ³digo acima)

**Testar:** `scrapy crawl aneel_noticias`

**Validar:** `python test_scrapers.py`

---

**Status:** PRONTO PARA IMPLEMENTAÃ‡ÃƒO! ğŸš€

**Powered by:** BMad Orchestrator + ness. ğŸ’™


